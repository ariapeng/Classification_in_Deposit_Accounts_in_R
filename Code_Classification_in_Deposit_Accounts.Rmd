---
title: "STA 6933 Advanced Topics in Statistical Learning"
author: "| Project 2\n| Spring 2022 \n| Marc Sandoval, Sanbrina Fautheree, Yahui Peng\n"
date: "5/10/2022"
output:
  pdf_document: default
  word_document: default
abstract: A bank compiled customer information during a campaign to identify the cause
  for revenue decline, which they concluded was caused by existing customers not obtaining
  long-term deposit accounts. In the project, we attempt to identify which customers
  are likely to obtain long-term deposit accounts based on customer information and
  demographics. Since the compiled data set mainly consists of categorical predictors,
  we will focus on tree-based methodologies in this project.
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm} \usepackage{float} \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, paged.print=FALSE)
```
# Overview
For this project, we are going to be looking at a banking data set to try and predict the likelihood of a customer obtaining a long-term deposit account based on their information and demographics. This data set has a total of 32,950 observations within it. We are looking at the data set from a business perspective. Meaning we are wanting to try a figure out which customers we have that are more likely to have a long-term deposit account. We will be examining the data through various tree-based modeling techniques considering a majority of predictor variables are categorical. Our analysis of data includes exploratory data analysis, pre-processing, feature selection, modeling, and performance evaluation of five different tree-based model types. These various techniques are what lead us to our conclusions.  

# Data Structure
The banking data consists of 15 predictors and 1 binary response variable, among which 10 predictors are nominal and 6 predictors contain nulls.
\newpage

```{r}
#plot results of script
library(kableExtra)
library(dplyr)
library(e1071)

#create table for data
pred_vars <- c("age","job","marital","education","default","housing","loan","contact","month", "dayofweek","duration","campaign","pdays","previous","poutcome")

Type <- c("numeric","nominal","nominal","nominal","nominal","nominal","nominal","nominal","nominal","nominal","numeric","numeric","numeric","numeric","nominal")
Null_ind <- c("no","yes","yes","yes","yes","yes","yes","no","no","no","no","no","no","no","no")

data_t <- data.frame(pred_vars, Type,Null_ind)

names(data_t) <- c("Predictor","Data Type","Contains Nulls")
knitr::kable(data_t, col.name = names(data_t),
             caption = "Predictor Variables") %>%
            kable_styling("striped")%>%
  kable_styling(latex_options = "hold_position")
```
  
As **pdays** represents the number of days that passed by after the client was last contacted from a previous campaign (999 means the client was not previously contacted). Therefore, for better visualization and interpretation, we encode **pdays** == 999 to NA for EDA purposes only, but they are immediately encoded back to 999 after EDA.  
Summary statistics and Box-plots suggest outliers exist for all predictors, however, tree-based models are the models of interest for this project and are outlier-insensitive. Therefore, we do not resolve the outliers.  

```{r}
# Load libraries ----------------------------------------------------------
library(caret)
library(Hmisc)
library(VIM) # aggr_plot() for value missing patterns

# Read the data -----------------------------------------------------------
data <- read.csv('D:/Yahui/Textbooks/STA6933_Adv_Stat_Learning/Project2/Banking Dataset Classification data.csv')

## Miscategorized data -----------------------------------------------------

# convert y to 0 if no; 1 if yes
data$y <- factor(data$y, levels =c("no","yes"), labels =c(0,1))

## Summary stats for numeric vars ----------------------------------
summary(data[,c("age","duration","campaign","pdays","previous")])

## Boxplots -----------------------------------------------------
col <- c("#61a2bc","#b93f1d",'#ab8fbe',"#d3a426","#88b998","#aaa197","#d67900","#cb7580")
data.numeric <- data[,c("age","duration","campaign","pdays","previous")]
par(mfrow=c(1,5)) 
for (i in 1:5) {
  boxplot(data.numeric[,i], main=colnames(data.numeric)[i],col=col[i])
}
```

Pairwise scatterplots, correlation matrix, and histograms for numeric preditors are shown below. First, in pairwise scatterplots, points are colored in red and grey for y = 1 and 0, respectively. We observe that clients with longer **duration** are more likely to have subscribed to a term deposit (y = 1). No obvious trend is observed between y and other numeric predictors. Second, The font size of the correlation coefficients displayed in the upper panel is proportional to the magnitude of the value. Therefore, no strong correlation is present between each pair of numeric predictors. Last, histograms show all numeric predictors are skewed.  

```{r}
## Pairwise scatterplots
panel.hist <- function(x, ...){
  #from help of pairs
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y,col = "#61a2bc")
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
  #from help of pairs
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y,use="pairwise.complete.obs"))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = 1.25*cex.cor * r,col="#402a34")
}

# pairs(data[,-9],col="steelblue")
pairs(data[,c("age","duration","campaign","pdays","previous")], 
      lower.panel = panel.smooth,
      upper.panel = panel.cor,col=ifelse(data$y==0, "#625a50", "firebrick"),
      diag.panel = panel.hist
)

data$pdays[is.na(data$pdays)] <- 999
```
  
Missing completely at random (MCAR) is the desirable scenario in case of missing data. Assuming data is MCAR, too much missing data can be a problem, too. Usually, a safe maximum threshold is 5% of the total for large datasets. The aggregation plot helps us understanding that almost 74% of the samples are not missing any information, 21% are missing the **default** value, and the remaining ones show other missing patterns. We probably should leave the feature **default** out. 
\newpage

![Missing Data Pattern](D:/Yahui/Textbooks/STA6933_Adv_Stat_Learning/Project2/Missing Data Pattern.jpg)

*Data Imputation*     

Our objective in imputing missing data for our categorical predictors was to retain the distribution of the variable after imputation. To achieve this, we apply the *mice()* function with the *random* method, which imputes the missing observations by replacing them with a random sample from the observed values. 

![Distribution on Predcitor Variables](D:/Yahui/Textbooks/STA6933_Adv_Stat_Learning/Project2/Imputation.jpg)

Bivariate analysis for ten categorical predictors is shown below. It is observed that **month** dec, mar, oct, and sep show a noticeable probability of y=1, which is nearly 50 %, indicating a possible lack of impact of these month categories on the response.  

```{r}
## Bivariate Analysis of Categorical predictors -----------------------------
library(ggplot2)
library(ggpubr)
pl1 <- ggplot(data = data, aes(x = as.factor(data$job), fill = y))+
  geom_bar()+
  xlab("job") +
  theme_classic()
pl2 <- ggplot(data = data, aes(x = as.factor(marital), fill = y))+
  geom_bar()+
  xlab("marital") +
  theme_classic()

pl3 <- ggplot(data = data, aes(x = as.factor(education), fill = y))+
  geom_bar()+
  xlab("education") +
  theme_classic()

pl4 <- ggplot(data = data, aes(as.factor(default),fill = y))+
  geom_bar()+
  xlab("default") +
  theme_classic()

pl5 <- ggplot(data = data, aes(as.factor(housing),fill = y))+
  geom_bar()+
  xlab("housing") +
  theme_classic()

pl6 <- ggplot(data = data, aes(as.factor(loan),fill = y))+
  geom_bar()+
  xlab("loan") +
  theme_classic()

pl7 <- ggplot(data = data, aes(as.factor(contact),fill = y))+
  geom_bar()+
  xlab("contact") +
  theme_classic()

pl8 <- ggplot(data = data, aes(as.factor(month),fill = y))+
  geom_bar()+
  xlab("month") +
  theme_classic()

pl9 <- ggplot(data = data, aes(as.factor(day_of_week),fill = y))+
  geom_bar()+
  xlab("day_of_week") +
  theme_classic()

pl10 <- ggplot(data = data, aes(as.factor(poutcome),fill = y))+
  geom_bar()+
  xlab("poutcome") +
  theme_classic()
```
```{r,fig.width=10}
pl1
cat("  \n")
cat("  \n")
cat("  \n")
pl3
```
```{r}
ggarrange(pl2, pl4, pl5, pl6, pl7, pl10, pl8, pl9, ncol = 2, nrow = 5, common.legend = TRUE, legend = "bottom")

# convert "unknown" to nulls
data[data=="unknown"] <- NA

# convert pdays = 999 to NA
data$pdays[data$pdays==999] <- -1
```


*Imbalanced Data*     
     
We observe that our response variable is imbalanced, which may cause poor model performance in the minority class. We applied the *SMOTE()* algorithm to create new observation in the minority class, which uses the k-nearest neighbor algorithm to generate new observation. The resulting training data set is more balanced.  

```{r,echo = FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#create table balance
Class <- c("Y=0","Y=1")
Cnt <- c("29,238","3,712")
prc <- c(0.887,0.113)

data_t <- data.frame(Class, Cnt,prc)

names(data_t) <- c("Class","Count","%")
knitr::kable(data_t, col.name = names(data_t),
             caption = "Response Variable Proportion") %>%
            kable_styling("striped")%>%
  kable_styling(latex_options = "hold_position")
```


```{r,echo = FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#create table balance
Class <- c("Y=0","Y=1")
Cnt <- c("23,391","17,820")
prc <- c(0.568,0.432)

data_t <- data.frame(Class, Cnt,prc)

names(data_t) <- c("Class","Count","%")
knitr::kable(data_t, col.name = names(data_t),
             caption = "Response Variable Proportion after Balancing") %>%
            kable_styling("striped")%>%
  kable_styling(latex_options = "hold_position")
```



```{r, eval=F}
## Feature Selection
load("D:/Yahui/Textbooks/STA6933_Adv_Stat_Learning/Project2/Project2_data.RData")
data_train$pdays[data_train$pdays == 999] <- NA # encode pday = 999 to NA then cap the outliers

# Resolve skewness ----
# Calculate the skewness of of each predictor.
apply(data_train[,c("age","duration","campaign","pdays","previous")], 2, function(x)skewness(na.omit(x)))

# It is observed that predictors *Insulin*, *DiabetesPedigreeFunction*, and *Age* are highly skewed.

# Box-cox transformation is used to deal with skewness and therefore might improve the classification model.
BoxCoxTrans(data_train$duration) # no transformation is applied
BoxCoxTrans(data_train$campaign) # take reciprocal, that is 1/campaign
BoxCoxTrans(data_train$pdays) # no transformation is applied
BoxCoxTrans(data_train$previous) # no transformation is applied
```

The skewness of numeric predictors are checked, and Box-Cox transformation suggests a reverse on campaign. Comparative histograms show that skewness is relieved after the transformation.   

```{r}
# plot comparison histograms
load("D:/Yahui/Textbooks/STA6933_Adv_Stat_Learning/Project2/Project2_data.RData")
par(mfrow=c(1,2))
hist(data_train$campaign, prob=T,xlab = "Campaign", col= "#9a8f83",main="Campaign (Before)")
hist(1/data_train$campaign, prob=T,xlab = "1/Campaign", col= "#cac4be",main="Campaign (After)")

data_train$rev.campaign <- 1/data_train$campaign
```
   
Near-zero variance predictors are further obtained. Considering that default have 21% missing in the original data, and has near-zero variance after imputation, we should consider drop it out. Additionally, job 3, 4, 6, 7, 9, 11, education 2 and 5, month 3, 9, 10, and 12 show near-zero variance.  

```{r}
# Near-zero variance vars -------------------------------------------------
load("D:/Yahui/Textbooks/STA6933_Adv_Stat_Learning/Project2/Project2_data.RData")
names(data_train)[nearZeroVar(data_train)] 
data_train$pdays[is.na(data_train$pdays)] <- 999
```

# Analysis

*Feature Selection*     

We utilized a logistic regression model to identify which predictors seem reasonable and used these predictors to construct our tree-based models. Based on multiple modeling iterations, we observed that the following combination of predictors resulted in a logistic regression model with all predictors significant at an $\alpha =0.001$ level: age, education, job, and marital status. In particular, the dummy variables for 9th grade education and university degrees were kept due to their significance. For job, the dummy variables for blue collar, retired, services, and student responded the best and will be retained. For martial status, the dummy variable for single was retained due to this analysis. In other words, this analysis provides evidence that 9th-grade or university educated, blue-collar/retired/service/student employed, and single customers appear to be more likely to open long-term deposit accounts.

*Models*  

Both summary of the baseline logistic regression model and variable importance of the random forest model suggest that age, education3, education7, job2, job6, job8, job9, and marital3 are informative predictors. Boosting model is further fitted using n.trees = 1000. Moreover, we fit BART and C5.0 classification models. The training and test errors as well as ROC curves are presented below.
```{r, eval=FALSE}
# Baseline model: logistic regression
base <- glm(y ~ age+education3+education7+job2+job6+job8+job9+marital3, data = data_train,  family=binomial())
summary(base)

log_out_tr <- as.factor(ifelse(predict(base,data_train,  type="response") >= 0.5,"1","0"))
log_out_te <- as.factor(ifelse(predict(base,data_test,  type="response") >= 0.5,"1","0"))
log_tr_err <- misClassError(data_train[,16], ifelse(log_out_tr == 1,1,0)) 
log_te_err <- misClassError(data_test[,16], ifelse(log_out_te == 1,1,0)) 
c("logistic regression model")
c("training error = ",log_tr_err)
c("testing error = ",log_te_err)

# Random forest
set.seed(1)
model1 <- randomForest(y ~ age+education3+education7+job2+job6+job8+job9+marital3, data = data_train, importance = TRUE)
rfImp1 <- varImp(model1, scale = FALSE)
rf_tr_pred = predict(model1, newdata = data_train,  n.trees = 100, type = "response")
rf_te_pred = predict(model1, newdata = data_test, n.trees = 100, type = "response")
c("Random Forest")
print(c("training error =",mean(data_train$y != rf_tr_pred, na.rm = TRUE)))
print(c("testing error =",mean(data_test$y != rf_te_pred, na.rm = TRUE)))


# Boosting Model
library(gbm)
set.seed(1)
# need response to be integer for boosted model
data_gbm_train <- data_train
data_gbm_train$y <- as.integer(levels(data_train[,16]))[data_train[,16]]
data_gbm_test <- data_test
data_gbm_test$y <- as.integer(levels(data_test[,16]))[data_test[,16]]

model2 = gbm(y ~ age+education3+education7+job2+job6+job8+job9+marital3, data = data_gbm_train, shrinkage=0.01, 
                distribution = 'bernoulli', n.trees = 1000, verbose=F)
boost_tr = predict(model2, newdata = data_gbm_train, n.trees = 1000, type = "response")
boost_te = predict(model2, newdata = data_gbm_test, n.trees = 1000, type = "response")
boost_tr_pred = ifelse(boost_tr>0.5,1,0)
boost_te_pred = ifelse(boost_te>0.5,1,0)
c("Boosting Model")
print(c("training error = ",mean(data_train$y != boost_tr_pred)))
print(c("testing error = ",mean(data_test$y != boost_te_pred)))

# BART Model
library(BART)
set.seed(1)
# Features are: age+education3+education7+job2+job6+job8+job9+marital3
data_tr_features <- data_gbm_train[,c(16,1,19,23,25,26,31,34,38)]
data_te_features <- data_gbm_test[,c(16,1,18,22,24,25,30,33,37)]

model4 = lbart(x.train = data_tr_features[,-1], y.train= data_tr_features[,1], 
               ndpost = 10, nskip = 10)
bart_tr_prob <- predict(model4, data_tr_features[,-1])$prob.test.mean
bart_te_prob <- predict(model4, data_te_features[,-1])$prob.test.mean

bart_tr_pred = as.factor(ifelse(bart_tr_prob>.5,1,0))
bart_te_pred = as.factor(ifelse(bart_te_prob>.5,1,0))
c("BART Model")
print(c("training error = ",mean(data_gbm_train$y != bart_tr_pred, na.rm = TRUE)))
print(c("testing error = ", mean(data_gbm_test$y != bart_te_pred, na.rm = TRUE)))

# C5.0 Model
library(C50)
set.seed(1)C
# Features are: age+education3+education7+job2+job6+job8+job9+marital3
data_tr_features <- data_gbm_train[,c(16,1,19,23,25,26,31,34,38)]
data_te_features <- data_gbm_test[,c(16,1,18,22,24,25,30,33,37)]

model5 <- C5.0(x = as.matrix(data_tr_features[,-1],rownames.force = NA), y = data_train$y)
c50_tr_pred = predict(model5, data_tr_features[,-1])
c50_te_pred = predict(model5, data_te_features[,-1])
c("C5.0 Model")
print(c("training error =",mean(data_train$y != c50_tr_pred, na.rm = TRUE)))
print(c("testing error =",mean(data_test$y != c50_te_pred, na.rm = TRUE)))
```

```{r, echo = FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#create table balance
model_type <- c("Logistic Model","Random Forest","Boosted Model", "BART Model", "C5.0 Model")
err_tr <- c("0.3262","0.1466","0.2182", "0.1421","0.1267")
err_te <- c("0.1490","0.1287","0.1283", "0.1279","0.1306")

data_t <- data.frame(model_type,err_tr,err_te)

names(data_t) <- c("Model","Training Error","Testing Error")
knitr::kable(data_t, col.name = names(data_t),
             caption = "Model Performance--Misclassification Error") %>%
            kable_styling("striped")%>%
  kable_styling(latex_options = "hold_position")
```

*ROC Curves*     

```{r, eval=TRUE, echo = FALSE, message=FALSE, warning=FALSE,fig.width=3, fig.height=3, paged.print=FALSE}
# ROC Curves
library(ggplot2)
library(pROC)
load("D:/Yahui/Textbooks/STA6933_Adv_Stat_Learning/Project2/predictions.RData")

# Logistic Model
rocobj <- roc(predictions$y, predictions$logistic)
auc <- round(auc(predictions$y, predictions$logistic),4)
ggroc(rocobj, colour = 'steelblue', size = 1) +
  ggtitle(paste0('ROC Curve: Logistic Model ', '(AUC = ', auc, ')')) +
  theme(plot.title = element_text(size = 8))

# Random Forest
rocobj <- roc(predictions$y, predictions$RF.1)
auc <- round(auc(predictions$y, predictions$RF.1),4)
ggroc(rocobj, colour = 'steelblue', size = 1) +
  ggtitle(paste0('ROC Curve: Random Forest ', '(AUC = ', auc, ')')) +
  theme(plot.title = element_text(size = 8))

# Boosted Model
rocobj <- roc(predictions$y, predictions$Boost)
auc <- round(auc(predictions$y, predictions$Boost),4)
ggroc(rocobj, colour = 'steelblue', size = 1) +
  ggtitle(paste0('ROC Curve: Boosted Model ', '(AUC = ', auc, ')')) +
  theme(plot.title = element_text(size = 8))

# BART Model
rocobj <- roc(predictions$y, predictions$BART)
auc <- round(auc(predictions$y, predictions$BART),4)
ggroc(rocobj, colour = 'steelblue', size = 1) +
  ggtitle(paste0('ROC Curve: BART Model ', '(AUC = ', auc, ')')) +
  theme(plot.title = element_text(size = 8))

# C5.0 Model
rocobj <- roc(predictions$y, predictions$C50.1)
auc <- round(auc(predictions$y, predictions$C50.1),4)
ggroc(rocobj, colour = 'steelblue', size = 1) +
  ggtitle(paste0('ROC Curve: C5.0 Model ', '(AUC = ', auc, ')')) +
  theme(plot.title = element_text(size = 8))
```

# Conclusion

In this project, we identified the customer demographics that were likely to result in long-term deposit accounts. In order to model the data, we performed data understanding and data pre-processing, such as imputation and balancing. We utilized tree-based methodologies since the majority of our predictors were categorical. We observed that the testing errors and AUC measures were similar across the 5 models we built, but the logistic regression model and boosted tree model had the highest training error, indicating these models were less flexible than the other 3 models. Based on the misclassification errors and AUC, it appears that the BART model performed the best for this data set.
